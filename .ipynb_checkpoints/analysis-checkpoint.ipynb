{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Support Quality Analysis\n",
        "\n",
        "Step-by-step analysis of the generated support chat dataset and LLM-based quality predictions.\n",
        "\n",
        "**Prerequisites:** Run `generate.py` and `analyze.py` first to produce `data/dataset.json` and `data/analysis.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    mean_absolute_error,\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with open('data/dataset.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "with open('data/analysis.json', 'r') as f:\n",
        "    analysis = json.load(f)\n",
        "\n",
        "analysis_map = {item['id']: item['analysis'] for item in analysis}\n",
        "\n",
        "rows = []\n",
        "for d in dataset:\n",
        "    gt = d['ground_truth']\n",
        "    pred = analysis_map.get(d['id'], {})\n",
        "    rows.append({\n",
        "        'id': d['id'],\n",
        "        'scenario_type': d['metadata']['scenario_type'],\n",
        "        'hidden_dissatisfaction': d['metadata']['has_hidden_dissatisfaction'],\n",
        "        'num_messages': len(d['messages']),\n",
        "        'gt_intent': gt['intent'],\n",
        "        'gt_satisfaction': gt['satisfaction'],\n",
        "        'gt_quality_score': gt['quality_score'],\n",
        "        'gt_mistakes': gt['agent_mistakes'],\n",
        "        'pred_intent': pred.get('intent', 'unknown'),\n",
        "        'pred_satisfaction': pred.get('satisfaction', 'unknown'),\n",
        "        'pred_quality_score': pred.get('quality_score', 0),\n",
        "        'pred_mistakes': pred.get('agent_mistakes', []),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(f'Total dialogs: {len(df)}')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "df['gt_intent'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Intent Distribution (Ground Truth)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "df['gt_satisfaction'].value_counts().plot(kind='bar', ax=axes[1], color=['green', 'orange', 'red'])\n",
        "axes[1].set_title('Satisfaction Distribution (Ground Truth)')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "df['scenario_type'].value_counts().plot(kind='bar', ax=axes[2], color='mediumpurple')\n",
        "axes[2].set_title('Scenario Type Distribution')\n",
        "axes[2].set_ylabel('Count')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nDialogs with hidden dissatisfaction: {df['hidden_dissatisfaction'].sum()}\")\n",
        "print(f\"Average messages per dialog: {df['num_messages'].mean():.1f}\")\n",
        "print(f\"\\nQuality score distribution (Ground Truth):\")\n",
        "print(df['gt_quality_score'].value_counts().sort_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Intent Classification: LLM vs Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "intent_labels = sorted(df['gt_intent'].unique())\n",
        "\n",
        "print('Intent Classification Report:')\n",
        "print(classification_report(df['gt_intent'], df['pred_intent'], labels=intent_labels, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(df['gt_intent'], df['pred_intent'], labels=intent_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=intent_labels, yticklabels=intent_labels)\n",
        "plt.title('Intent Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Ground Truth')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "intent_acc = accuracy_score(df['gt_intent'], df['pred_intent'])\n",
        "print(f'\\nIntent Accuracy: {intent_acc:.2%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Satisfaction Prediction: LLM vs Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sat_labels = ['satisfied', 'neutral', 'unsatisfied']\n",
        "\n",
        "print('Satisfaction Classification Report:')\n",
        "print(classification_report(df['gt_satisfaction'], df['pred_satisfaction'],\n",
        "                            labels=sat_labels, zero_division=0))\n",
        "\n",
        "cm_sat = confusion_matrix(df['gt_satisfaction'], df['pred_satisfaction'], labels=sat_labels)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_sat, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=sat_labels, yticklabels=sat_labels)\n",
        "plt.title('Satisfaction Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Ground Truth')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "sat_acc = accuracy_score(df['gt_satisfaction'], df['pred_satisfaction'])\n",
        "print(f'\\nSatisfaction Accuracy: {sat_acc:.2%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quality Score Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].scatter(df['gt_quality_score'], df['pred_quality_score'], alpha=0.5, s=80)\n",
        "axes[0].plot([1, 5], [1, 5], 'r--', label='Perfect prediction')\n",
        "axes[0].set_xlabel('Ground Truth Quality Score')\n",
        "axes[0].set_ylabel('Predicted Quality Score')\n",
        "axes[0].set_title('Quality Score: Predicted vs Ground Truth')\n",
        "axes[0].legend()\n",
        "axes[0].set_xticks([1, 2, 3, 4, 5])\n",
        "axes[0].set_yticks([1, 2, 3, 4, 5])\n",
        "\n",
        "score_diff = df['pred_quality_score'] - df['gt_quality_score']\n",
        "score_diff.plot(kind='hist', bins=range(-5, 6), ax=axes[1], color='teal', edgecolor='black')\n",
        "axes[1].set_xlabel('Prediction Error (Predicted - Ground Truth)')\n",
        "axes[1].set_title('Quality Score Error Distribution')\n",
        "axes[1].axvline(x=0, color='red', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "mae = mean_absolute_error(df['gt_quality_score'], df['pred_quality_score'])\n",
        "exact_match = (df['gt_quality_score'] == df['pred_quality_score']).mean()\n",
        "within_one = (abs(df['gt_quality_score'] - df['pred_quality_score']) <= 1).mean()\n",
        "correlation = df['gt_quality_score'].corr(df['pred_quality_score'])\n",
        "\n",
        "print(f'Mean Absolute Error: {mae:.2f}')\n",
        "print(f'Exact Match: {exact_match:.2%}')\n",
        "print(f'Within +-1: {within_one:.2%}')\n",
        "print(f'Correlation: {correlation:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Agent Mistakes Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_mistakes = ['ignored_question', 'incorrect_info', 'rude_tone', 'no_resolution', 'unnecessary_escalation']\n",
        "\n",
        "mistake_metrics = []\n",
        "for mistake in all_mistakes:\n",
        "    gt_has = df['gt_mistakes'].apply(lambda x: mistake in x)\n",
        "    pred_has = df['pred_mistakes'].apply(lambda x: mistake in x)\n",
        "\n",
        "    tp = (gt_has & pred_has).sum()\n",
        "    fp = (~gt_has & pred_has).sum()\n",
        "    fn = (gt_has & ~pred_has).sum()\n",
        "    tn = (~gt_has & ~pred_has).sum()\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    mistake_metrics.append({\n",
        "        'mistake': mistake,\n",
        "        'gt_count': gt_has.sum(),\n",
        "        'pred_count': pred_has.sum(),\n",
        "        'TP': tp, 'FP': fp, 'FN': fn,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    })\n",
        "\n",
        "mistakes_df = pd.DataFrame(mistake_metrics)\n",
        "print('Agent Mistakes Detection Metrics:')\n",
        "print(mistakes_df.to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x = range(len(all_mistakes))\n",
        "width = 0.25\n",
        "ax.bar([i - width for i in x], mistakes_df['precision'], width, label='Precision', color='steelblue')\n",
        "ax.bar(x, mistakes_df['recall'], width, label='Recall', color='coral')\n",
        "ax.bar([i + width for i in x], mistakes_df['f1'], width, label='F1', color='green')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(all_mistakes, rotation=30, ha='right')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Agent Mistake Detection: Precision / Recall / F1')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Anomaly Detection: Hidden Dissatisfaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hidden = df[df['hidden_dissatisfaction'] == True].copy()\n",
        "print(f'Total dialogs with hidden dissatisfaction: {len(hidden)}')\n",
        "print()\n",
        "\n",
        "hidden_detected = hidden[hidden['pred_satisfaction'] == 'unsatisfied']\n",
        "hidden_missed = hidden[hidden['pred_satisfaction'] != 'unsatisfied']\n",
        "\n",
        "detection_rate = len(hidden_detected) / len(hidden) if len(hidden) > 0 else 0\n",
        "print(f'LLM correctly detected hidden dissatisfaction: {len(hidden_detected)}/{len(hidden)} ({detection_rate:.0%})')\n",
        "print(f'LLM missed (predicted satisfied/neutral): {len(hidden_missed)}/{len(hidden)}')\n",
        "\n",
        "if len(hidden_missed) > 0:\n",
        "    print('\\nMissed cases (LLM thought customer was satisfied/neutral):')\n",
        "    for _, row in hidden_missed.iterrows():\n",
        "        print(f'  Dialog {row[\"id\"]}: predicted={row[\"pred_satisfaction\"]}, '\n",
        "              f'intent={row[\"gt_intent\"]}, scenario={row[\"scenario_type\"]}')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "hidden['pred_satisfaction'].value_counts().plot(kind='bar', ax=ax, color=['red', 'orange', 'green'])\n",
        "ax.set_title('LLM Predictions for Hidden Dissatisfaction Cases')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_xlabel('Predicted Satisfaction')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Disagreement Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['intent_match'] = df['gt_intent'] == df['pred_intent']\n",
        "df['satisfaction_match'] = df['gt_satisfaction'] == df['pred_satisfaction']\n",
        "df['quality_diff'] = abs(df['gt_quality_score'] - df['pred_quality_score'])\n",
        "\n",
        "disagreements = df[\n",
        "    (~df['intent_match']) | (~df['satisfaction_match']) | (df['quality_diff'] >= 2)\n",
        "].copy()\n",
        "\n",
        "print(f'Total disagreements (intent OR satisfaction mismatch OR quality diff >= 2): {len(disagreements)}')\n",
        "print(f'  Intent mismatches: {(~df[\"intent_match\"]).sum()}')\n",
        "print(f'  Satisfaction mismatches: {(~df[\"satisfaction_match\"]).sum()}')\n",
        "print(f'  Quality score diff >= 2: {(df[\"quality_diff\"] >= 2).sum()}')\n",
        "\n",
        "if len(disagreements) > 0:\n",
        "    print('\\nSample disagreements:')\n",
        "    cols = ['id', 'scenario_type', 'gt_intent', 'pred_intent',\n",
        "            'gt_satisfaction', 'pred_satisfaction', 'gt_quality_score', 'pred_quality_score']\n",
        "    print(disagreements[cols].head(15).to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "disagree_by_type = disagreements['scenario_type'].value_counts()\n",
        "all_by_type = df['scenario_type'].value_counts()\n",
        "disagree_rate = (disagree_by_type / all_by_type).fillna(0)\n",
        "disagree_rate.plot(kind='bar', ax=ax, color='salmon')\n",
        "ax.set_title('Disagreement Rate by Scenario Type')\n",
        "ax.set_ylabel('Disagreement Rate')\n",
        "ax.set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('=' * 60)\n",
        "print('ANALYSIS SUMMARY')\n",
        "print('=' * 60)\n",
        "print(f'Total dialogs analyzed: {len(df)}')\n",
        "print(f'\\nIntent classification accuracy: {accuracy_score(df[\"gt_intent\"], df[\"pred_intent\"]):.2%}')\n",
        "print(f'Satisfaction classification accuracy: {accuracy_score(df[\"gt_satisfaction\"], df[\"pred_satisfaction\"]):.2%}')\n",
        "print(f'Quality score MAE: {mean_absolute_error(df[\"gt_quality_score\"], df[\"pred_quality_score\"]):.2f}')\n",
        "print(f'Quality score correlation: {df[\"gt_quality_score\"].corr(df[\"pred_quality_score\"]):.3f}')\n",
        "print(f'\\nHidden dissatisfaction detection rate: {detection_rate:.0%}')\n",
        "print(f'Total disagreements: {len(disagreements)} / {len(df)}')\n",
        "print(f'\\nAgent mistake detection (avg F1): {mistakes_df[\"f1\"].mean():.2f}')\n",
        "print('=' * 60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
